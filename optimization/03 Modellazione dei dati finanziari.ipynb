{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad04dc0",
   "metadata": {},
   "source": [
    "# Dati Finanziari: Modellazione I.I.D.\n",
    "\n",
    "> \"Tutti i modelli sono sbagliati, ma alcuni sono utili.\" — George E. P. Box\n",
    "\n",
    "Sotto l'ipotesi del mercato efficiente (Fama, 1970), il prezzo di un titolo è una buona stima del suo valore intrinseco. \n",
    "\n",
    "Cioè, qualsiasi informazione sulle prospettive future è già incorporata nel prezzo corrente, quindi la previsione è solo il prezzo corrente. Questo porta a modellare i prezzi come un cammino casuale (Malkiel, 1973) e, equivalentemente, i rendimenti come una sequenza di variabili casuali indipendenti e identicamente distribuite (i.i.d.). Nel caso di più asset, le variabili casuali denotano i rendimenti di tutti gli asset, portando a una variabile casuale multivariata. Questo è un modello semplice e conveniente, che in realtà è stato già impiegato nel documento seminale di Markowitz sulla progettazione del portafoglio (Markowitz, 1952).\n",
    "\n",
    "Sotto il modello i.i.d., nessuna struttura temporale è incorporata e i rendimenti in un dato momento sono considerati indipendenti da altri istanti temporali; inoltre, la distribuzione dei rendimenti casuali nel tempo è considerata fissa. Da qui la terminologia \"indipendenti e identicamente distribuiti\". Questo capitolo esplora la caratterizzazione della distribuzione multivariata i.i.d., dai più semplici stimatori campionari ai più sofisticati stimatori robusti non gaussiani che possono includere informazioni preliminari sotto forma di riduzione, modellazione fattoriale o visioni preliminari.\n",
    "\n",
    "## Modello I.I.D.\n",
    "\n",
    "Il capitolo offre una visione esplorativa dei dati finanziari e dei fatti stilizzati importanti. \n",
    "\n",
    "Questo capitolo esplora un modo semplice ma utile e popolare per modellare i dati finanziari. Supponiamo di avere  \n",
    "$N$  titoli o asset negoziabili – possibilmente da classi di asset distinte come obbligazioni, azioni, materie prime, fondi comuni di investimento, valute e criptovalute – e indichiamo  $x_t \\in \\mathbb{R}^N$ \n",
    "(spesso denotato come $r_t$) i rendimenti casuali degli asset al tempo $t$. Si noti che l'indice temporale  \n",
    "$t$ può denotare qualsiasi periodo arbitrario come minuti, ore, giorni, settimane, mesi, trimestri, anni, e così via.\n",
    "\n",
    "Sotto il modello i.i.d., i rendimenti sono semplicemente modellati come $x_t = \\mu + \\epsilon_t$,  \n",
    "dove  $\\mu \\in \\mathbb{R}^N$ denota il rendimento atteso e $\\epsilon_t \\in \\mathbb{R}^N$ è la componente residua con media zero e matrice di covarianza $\\Sigma \\in \\mathbb{R}^{N \\times N}$. \n",
    "\n",
    "Questo modello può essere motivato dall'ipotesi del mercato efficiente (Fama, 1970). La Figura 3.1 mostra un esempio di una serie temporale i.i.d. gaussiana univariata sintetica.\n",
    "\n",
    "![random walk](./images/iid-time-series-1.png)\n",
    "\n",
    "Il testo spiega che il modello i.i.d. (indipendente e identicamente distribuito) descritto nell'equazione (3.1) corrisponde al modello del cammino casuale sui log-prezzi. Questo è anche noto come modello del cammino casuale geometrico sui prezzi.\n",
    "\n",
    "### Dettagli del Modello\n",
    "\n",
    "- **Log-Prezzi**: I log-prezzi sono definiti come $y_t \\triangleq \\log p_t$, dove $p_t$ rappresenta il prezzo dell'asset al tempo $t$.\n",
    "- **Modello del Cammino Casuale**: Il modello del cammino casuale sui log-prezzi è espresso come:\n",
    "  $y_t = \\mu + y_{t-1} + \\epsilon_t$\n",
    "  dove:\n",
    "  - $\\mu$ è il rendimento atteso.\n",
    "  - $y_{t-1}$ è il log-prezzo al tempo precedente.\n",
    "  - $\\epsilon_t$ è la componente residua con media zero.\n",
    "\n",
    "### Connessione con il Modello i.i.d.\n",
    "\n",
    "- **Log-Rendimenti**: I log-rendimenti sono definiti come la differenza tra i log-prezzi successivi:\n",
    "  $x_t = y_t - y_{t-1}$\n",
    "- **Equazione (3.1)**: Sostituendo l'espressione dei log-rendimenti nel modello del cammino casuale, otteniamo:\n",
    "  $x_t = \\mu + \\epsilon_t$\n",
    "  Questo è esattamente il modello i.i.d. descritto nell'equazione (3.1).\n",
    "\n",
    "### Contesto Circostante\n",
    "\n",
    "- **Ipotesi del Mercato Efficiente**: Il modello i.i.d. può essere motivato dall'ipotesi del mercato efficiente, che suggerisce che i prezzi riflettono tutte le informazioni disponibili.\n",
    "- **Modelli di Serie Temporali**: Il modello i.i.d. ignora qualsiasi struttura temporale nei dati. Capitoli successivi esplorano modelli di serie temporali più sofisticati che tentano di incorporare la struttura temporale.\n",
    "\n",
    "## Semplici Stimatori\n",
    "\n",
    "\n",
    "Nella pratica, i parametri del modello i.i.d. in (3.1), $(\\mu, \\Sigma)$, sono sconosciuti e devono essere stimati utilizzando dati storici $x_1, \\ldots, x_T$ contenenti $T$ osservazioni passate. I più semplici stimatori sono la media campionaria,\n",
    "\n",
    "$\n",
    "\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^{T} x_t,\n",
    "$\n",
    "\n",
    "e la matrice di covarianza campionaria,\n",
    "\n",
    "$\n",
    "\\hat{\\Sigma} = \\frac{1}{T-1} \\sum_{t=1}^{T} (x_t - \\hat{\\mu})(x_t - \\hat{\\mu})^T,\n",
    "$\n",
    "\n",
    "dove la notazione con il \"cappello\" $\\hat{}$ denota la stima. Una proprietà importante è che questi stimatori sono non distorti, cioè,\n",
    "\n",
    "$\n",
    "IE[\\hat{\\mu}] = \\mu, \\quad IE[\\hat{\\Sigma}] = \\Sigma.\n",
    "$\n",
    "\n",
    "In altre parole, le stime casuali $\\hat{\\mu}$ e $\\hat{\\Sigma}$ sono centrate sui valori veri. Infatti, il fattore di $\\frac{1}{T-1}$ nella matrice di covarianza campionaria è scelto proprio per rendere lo stimatore non distorto; se invece si usa il fattore più naturale $\\frac{1}{T}$, allora lo stimatore è distorto:\n",
    "\n",
    "$\n",
    "IE[\\hat{\\Sigma}] = \\left(1 - \\frac{1}{T}\\right) \\Sigma.\n",
    "$\n",
    "\n",
    "Un'altra proprietà importante è che questi stimatori sono consistenti, cioè, dalla legge dei grandi numeri (T. W. Anderson, 2003; Papoulis, 1991) segue che\n",
    "\n",
    "$$\n",
    "\\lim_{T \\to \\infty} \\hat{\\mu} = \\mu, \\quad \\lim_{T \\to \\infty} \\hat{\\Sigma} = \\Sigma.\n",
    "$$\n",
    "\n",
    "In altre parole, le stime campionarie convergono alle quantità vere man mano che il numero di osservazioni $T$ cresce. La Figura 3.2 mostra come l'errore di stima effettivamente si riduce a zero man mano che $T$ cresce per dati sintetici gaussiani di dimensione $N = 100$ (in particolare, viene utilizzato l'errore normalizzato, definito come $100 \\times \\frac{\\|\\hat{\\mu} - \\mu\\|}{\\|\\mu\\|}$ per il caso della media campionaria, e similmente per la matrice di covarianza campionaria).\n",
    "\n",
    "**Figura 3.2:** Errore di stima degli stimatori campionari vs. numero di osservazioni (per dati gaussiani con $N = 100$).\n",
    "\n",
    "\n",
    "![stime](./images/sample-estimators-vs-T-1.png)\n",
    "\n",
    "\n",
    "\n",
    "Questi stimatori campionari $\\hat{\\mu}$ e $\\hat{\\Sigma}$ sono facili da comprendere, semplici da implementare e poco costosi in termini di calcolo. Tuttavia, sono buoni stimatori solo per un gran numero di osservazioni $T$; altrimenti, l'errore di stima diventa inaccettabile. In particolare, la media campionaria è uno stimatore molto inefficiente, producendo stime molto rumorose (Chopra e Ziemba, 1993; Meucci, 2005). Questo può essere osservato nella Figura 3.2, dove l'errore normalizzato di $\\hat{\\mu}$ per $N = 100$ e $T = 500$ è superiore al 100%, il che significa che l'errore è grande quanto il vero $\\mu$.\n",
    "\n",
    "Nella pratica, tuttavia, ci sono due ragioni principali per cui $T$ non può essere scelto abbastanza grande da produrre buone stime:\n",
    "\n",
    "1. **Mancanza di dati storici disponibili:** Ad esempio, se usiamo dati azionari giornalieri (cioè, 252 osservazioni all'anno) e la dimensione dell'universo è, diciamo, $N = 500$, allora una regola empirica popolare suggerisce che dovremmo usare $T \\approx 10 \\times N$ osservazioni, il che significa 20 anni di dati. Generalmente parlando, 20 anni di dati sono raramente disponibili per l'intero universo di asset.\n",
    "2. **Mancanza di stazionarietà:** Anche se avessimo a disposizione tutti i dati storici desiderati, poiché i dati finanziari non sono stazionari su lunghi periodi di tempo, non ha molto senso usare tali dati: il comportamento e le dinamiche del mercato di 20 anni fa sono troppo diversi da quelli attuali (cfr. Capitolo 2).\n",
    "\n",
    "Di conseguenza, in un contesto pratico, la quantità di dati che può essere utilizzata è molto limitata. Ma allora, le stime $\\hat{\\mu}$ e $\\hat{\\Sigma}$ saranno molto rumorose, in particolare la media campionaria. \n",
    "\n",
    "Questo è, infatti, il \"tallone d'Achille\" dell'ottimizzazione del portafoglio: $\\hat{\\mu}$ e $\\hat{\\Sigma}$ conterranno inevitabilmente rumore di stima che porterà a progetti di portafoglio erratici. Questo è il motivo per cui il portafoglio di Markowitz non è stato pienamente abbracciato dai professionisti.\n",
    "\n",
    "Nel resto di questo capitolo, otterremo una prospettiva più profonda sul perché gli stimatori campionari funzionano così male con i dati finanziari e, poi, esploreremo diversi modi per migliorarli.\n",
    "\n",
    "\n",
    "## Stimatori di Posizione\n",
    "\n",
    "Dal modello i.i.d. in (3.1), il parametro $\\mu$ può essere interpretato come il \"centro\" o \"posizione\" attorno al quale i punti casuali sono distribuiti. Pertanto, ha senso cercare di stimare questa posizione, cosa che può essere fatta in vari modi. L'approccio classico si basa sull'adattamento dei minimi quadrati, ma è molto sensibile alle osservazioni estreme e ai valori mancanti; quindi, stimatori di posizione multivariata robusti alternativi sono stati ampiamente studiati nella letteratura. Infatti, questo argomento può essere fatto risalire agli anni '60 (Huber, 1964; Maronna, 1976), come riportato in Small (1990), Huber (2011) e Oja (2013).\n",
    "\n",
    "\n",
    "### Stimatore dei Minimi Quadrati\n",
    "\n",
    "Il metodo dei minimi quadrati (LS) risale al 1795, quando Gauss lo utilizzò per studiare i moti planetari. Si occupa del modello lineare $y = Ax + \\epsilon$ risolvendo il problema (Kay, 1993; Scharf, 1991)\n",
    "\n",
    "$\\min_x \\| y - Ax \\|_2^2,$\n",
    "\n",
    "che ha la soluzione in forma chiusa\n",
    "\n",
    "$x^\\star = (A^T A)^{-1} A^T y.$\n",
    "\n",
    "Tornando al modello i.i.d. in (3.1), possiamo ora formulare la stima del centro dei punti, $x_t = \\mu + \\epsilon_t,$ come un problema di LS:\n",
    "\n",
    "$\\min_\\mu \\sum_{t=1}^T \\| x_t - \\mu \\|_2^2,$\n",
    "\n",
    "la cui soluzione, curiosamente, coincide con la media campionaria in (3.2):\n",
    "\n",
    "$\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^T x_t.$\n",
    "\n",
    "La media campionaria è ben nota per soffrire di mancanza di robustezza contro punti contaminati o outliers, così come contro distribuzioni con code pesanti (come sarà verificato empiricamente più avanti nella Figura 3.4).\n",
    "\n",
    "\n",
    "### Stimatore della Mediana Spaziale\n",
    "\n",
    "Un altro modo per estendere la mediana univariata al caso multivariato è la cosiddetta mediana spaziale o mediana geometrica (nota anche come mediana $L_1$), ottenuta come soluzione del problema\n",
    "\n",
    "$\\min_\\mu \\sum_{t=1}^T \\| x_t - \\mu \\|_2,$\n",
    "\n",
    "dove ora la norma $\\ell_2$ o distanza euclidea è la misura dell'errore invece della norma $\\ell_1$ o della norma $\\ell_2$ al quadrato. Interessantemente, la mancanza dell'operatore di elevazione al quadrato ha un grande effetto; ad esempio, lo stimatore per ciascun elemento non è più indipendente dagli altri elementi (come nel caso della media campionaria e della mediana elemento per elemento). La mediana spaziale ha la proprietà desiderata che per $N = 1$ si riduce alla mediana univariata.\n",
    "\n",
    "La mediana spaziale è la soluzione di un problema convesso di cono di secondo ordine (SOCP) e può essere risolta con un risolutore SOCP. In alternativa, algoritmi iterativi molto efficienti possono essere derivati risolvendo una sequenza di problemi LS tramite il metodo di maggiorazione-minimizzazione (MM) (Y. Sun et al., 2017).\n",
    "\n",
    "\n",
    "### Esperimenti Numerici\n",
    "\n",
    "La Figura 3.3 illustra i diversi stimatori di posizione in un caso bidimensionale ($N = 2$). Poiché questa è solo un test, non possiamo concludere quale metodo sia preferibile.\n",
    "\n",
    "![stime](./images/scatter-plots-location-methods-1.png)\n",
    "\n",
    "\n",
    "La Figura 3.4 mostra l'errore di stima di diversi stimatori di posizione in funzione del numero di osservazioni $T$ per dati sintetici Gaussiani e con code pesanti (il vettore medio $\\mu$ e la matrice di covarianza $\\Sigma$ sono presi come misurati nei dati tipici del mercato azionario). Per i dati Gaussiani, la media campionaria è il miglior stimatore (come ulteriormente analizzato nella Sezione 3.4), con la mediana spaziale quasi identica, e la mediana elemento per elemento la peggiore. Per i dati con code pesanti, la media campionaria è tanto pessima quanto la mediana e la mediana spaziale rimane la migliore. Complessivamente, la mediana spaziale sembra essere l'opzione migliore poiché è robusta agli outliers e non peggiora con i dati Gaussiani.\n",
    "\n",
    "![stime](./images/location-estimators-vs-T-1.png)\n",
    "\n",
    "\n",
    "La Figura 3.5 esamina in dettaglio l'effetto delle code pesanti sull'errore di stima per dati sintetici che seguono una distribuzione $t$ con gradi di libertà $\\nu$. I dati finanziari tipicamente corrispondono a $\\nu$ dell'ordine di 4 o 5, che è significativamente a coda pesante, mentre un valore grande di $\\nu$ corrisponde a una distribuzione Gaussiana. Possiamo osservare che l'errore per la media campionaria rimane simile indipendentemente da $\\nu$. Interessantemente, la mediana spaziale è simile alla media campionaria per grandi $\\nu$ (regime Gaussiano) mentre diventa migliore per code pesanti. Questo illustra nuovamente la robustezza della mediana spaziale.\n",
    "\n",
    "![stime](./images/location-estimators-vs-nu-1.png)\n",
    "\n",
    "\n",
    "\n",
    "## Stimatori ML Gaussiani\n",
    "\n",
    "### Preliminari sulla Stima ML\n",
    "\n",
    "La stima di massima verosimiglianza (ML) è una tecnica importante nella teoria della stima (T. W. Anderson, 2003; Kay, 1993; Scharf, 1991). L'idea è molto semplice. Supponiamo che la probabilità di una variabile casuale $x$ possa essere \"misurata\" dalla funzione di distribuzione di probabilità (pdf) $f$. Allora la probabilità di una serie di $T$ osservazioni indipendenti $x_1, \\ldots, x_T$ può essere misurata dal prodotto $f(x_1) \\times \\cdots \\times f(x_T)$. Supponiamo ora di dover indovinare quale delle due possibili distribuzioni, $f_1$ e $f_2$, è più probabile che abbia prodotto queste osservazioni. Sembra ragionevole scegliere come più probabile quella che dà la maggiore probabilità di osservare queste osservazioni. Ora, supponiamo di avere una famiglia di possibili distribuzioni parametrizzate dal vettore di parametri $\\theta$: $f_\\theta$, che è chiamata funzione di verosimiglianza quando vista come una funzione di $\\theta$ per le osservazioni date. \n",
    "\n",
    "Ancora una volta, sembra ragionevole scegliere come più probabile $\\theta$ quella che dà la maggiore probabilità di osservare queste osservazioni. Questa è precisamente l'essenza della stima ML e può essere scritta come il seguente problema di ottimizzazione:\n",
    "\n",
    "$\\max_\\theta f_\\theta(x_1) \\times \\cdots \\times f_\\theta(x_T),$\n",
    "\n",
    "dove $x_1, \\ldots, x_T$ denotano le $T$ osservazioni date. \n",
    "\n",
    "Per comodità matematica, la stima ML è formulata in modo equivalente come la massimizzazione della funzione di log-verosimiglianza:\n",
    "\n",
    "$\\max_\\theta \\sum_{t=1}^T \\log f_\\theta(x_t).$\n",
    "\n",
    "Lo stimatore ML (MLE) gode di molte proprietà teoriche desiderabili che lo rendono uno stimatore asintoticamente ottimale (asintotico nel numero di osservazioni $T$). Più specificamente, l'MLE è asintoticamente non distorto e raggiunge asintoticamente il limite di Cramér-Rao (che dà la varianza più bassa possibile raggiungibile da uno stimatore non distorto); in altre parole, è asintoticamente efficiente (Kay, 1993; Scharf, 1991). In pratica, tuttavia, la domanda chiave è quanto grande deve essere $T$ affinché le proprietà asintotiche si mantengano effettivamente.\n",
    "\n",
    "### Stima ML Gaussiana\n",
    "\n",
    "La pdf corrispondente al modello i.i.d. (3.1), assumendo che il residuo segua una distribuzione normale multivariata o gaussiana, è\n",
    "\n",
    "$\n",
    "f(x) = \\frac{1}{\\sqrt{(2\\pi)^N |\\Sigma|}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right),\n",
    "$\n",
    "\n",
    "\n",
    "da cui possiamo vedere che i parametri del modello sono $\\theta = (\\mu, \\Sigma).$\n",
    "Dato $T$ osservazioni $x_1, \\ldots, x_T$, l'MLE può essere formulato (ignorando i termini costanti irrilevanti) come\n",
    "\n",
    "$\n",
    "\\min_{\\mu, \\Sigma} \\log \\det(\\Sigma) + \\frac{1}{T} \\sum_{t=1}^T (x_t - \\mu)^T \\Sigma^{-1} (x_t - \\mu),\n",
    "$\n",
    "\n",
    "dove $\\log \\det(\\cdot)$ denota il logaritmo del determinante di una matrice.\n",
    "Impostando il gradiente della funzione obiettivo rispetto a $\\mu$ e $\\Sigma^{-1}$ a zero si ottiene\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^T (x - \\mu) = 0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "-\\Sigma + \\frac{1}{T} \\sum_{t=1}^T (x_t - \\mu)(x_t - \\mu)^T = 0,\n",
    "$$\n",
    "\n",
    "che porta ai seguenti stimatori per $\\mu$ e $\\Sigma$:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^T x_t,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\Sigma} = \\frac{1}{T} \\sum_{t=1}^T (x_t - \\mu)(x_t - \\mu)^T.\n",
    "$$\n",
    "\n",
    "\n",
    "Interessantemente, questi stimatori coincidono con gli stimatori campionari in (3.2) e (3.3), a parte il fattore $1/T$ invece del fattore $1/(T-1)$. Lo stimatore ML della matrice di covarianza è distorto poiché\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\Sigma}] = \\left(1 - \\frac{1}{T}\\right)\\Sigma;\n",
    "$$\n",
    "\n",
    "tuttavia, è asintoticamente non distorto quando $T \\to \\infty$ (come già previsto dall'ottimalità asintotica di ML).\n",
    "\n",
    "All'inizio, potrebbe sembrare che non abbiamo ottenuto nulla di nuovo, poiché avevamo già derivato gli stessi stimatori dalla prospettiva degli stimatori campionari e dalla prospettiva dei minimi quadrati. Tuttavia, dopo una riflessione più attenta, abbiamo effettivamente appreso che gli stimatori campionari sono ottimali quando i dati sono distribuiti secondo una Gaussiana, ma non altrimenti. Cioè, quando i dati hanno una distribuzione diversa, ci si può aspettare che gli stimatori ML ottimali siano diversi, come esplorato più avanti nella Sezione 3.5.\n",
    "\n",
    "### Esperimenti Numerici\n",
    "\n",
    "La Figura 3.6 mostra l'errore di stima degli stimatori ML per la media e la matrice di covarianza in funzione del numero di osservazioni $T$ per dati sintetici Gaussiani e con code pesanti. Possiamo osservare come la stima della matrice di covarianza $\\Sigma$ peggiori notevolmente per i dati con code pesanti, mentre la stima della media $\\mu$ rimane simile.\n",
    "\n",
    "\n",
    "![stime](./images/Gaussian-ML-estimators-vs-T-1.png)\n",
    "\n",
    "\n",
    "La Figura 3.7 esamina in maggior dettaglio l'effetto delle code pesanti sull'errore di stima per dati sintetici che seguono una distribuzione $t$ con gradi di libertà $\\nu$. Possiamo confermare che l'errore nella stima della media rimane lo stesso, mentre la stima della matrice di covarianza peggiora man mano che la distribuzione presenta code più pesanti. Tuttavia, non dobbiamo dimenticare che la dimensione dell'errore è di un ordine di grandezza maggiore per la media rispetto alla matrice di covarianza.\n",
    "\n",
    "![stime](./images/Gaussian-ML-estimators-vs-nu-1.png)\n",
    "\n",
    "## Stimatori ML con Code Pesanti\n",
    "\n",
    "Gli stimatori ML Gaussiani sono ottimali, nel senso che massimizzano la verosimiglianza delle osservazioni, ogni volta che i dati seguono la distribuzione Gaussiana. Tuttavia, se i dati seguono una distribuzione con code pesanti – come nel caso dei dati finanziari – allora dobbiamo comprendere meglio il potenziale effetto dannoso. Da un lato, poiché gli stimatori ML coincidono con gli stimatori campionari nella Sezione 3.2, sappiamo che sono non distorti e consistenti, che sono proprietà desiderabili. Ma, dall'altro lato, è sufficiente o possiamo fare di meglio?\n",
    "\n",
    "\n",
    "### Il Fallimento degli Stimatori ML Gaussiani\n",
    "\n",
    "Come esplorato nella Sezione 3.4, la Figura 3.6 dimostra che l'effetto delle code pesanti nella stima della matrice di covarianza è significativo, mentre è quasi inesistente per la stima della media. La Figura 3.7 mostra ulteriormente l'errore in funzione di quanto sono pesanti le code (un piccolo $\\nu$ rappresenta distribuzioni con code pesanti mentre un grande $\\nu$ tende a una distribuzione Gaussiana).\n",
    "\n",
    "Per comprendere meglio l'effetto dannoso delle code pesanti, la Figura 3.8 illustra questo effetto, così come l'effetto degli outlier in dati altrimenti Gaussiani. In questo esempio, $T = 10$ punti dati sono utilizzati per la stima della matrice di covarianza bidimensionale, che soddisfa il rapporto $T/N = 5$. È molto chiaro che, mentre per i dati Gaussiani lo stimatore ML Gaussiano (o matrice di covarianza campionaria) segue molto da vicino la vera matrice di covarianza, una volta che includiamo un singolo outlier nei dati Gaussiani o utilizziamo dati con code pesanti, la stima è disastrosa. Tuttavia, la maggior parte dei professionisti e degli accademici adotta la matrice di covarianza campionaria per la sua semplicità.\n",
    "\n",
    "![stime](./images/scatter-plots-Gaussian-MLE-1.png)\n",
    "\n",
    "\n",
    "## Stima ML con Code Pesanti\n",
    "\n",
    "Abbiamo osservato empiricamente che il massimo di verosimiglianza (MLE) gaussiano per la matrice di covarianza degrada significativamente quando la distribuzione dei dati presenta code pesanti. Questo non è sorprendente, poiché la matrice di covarianza campionaria è ottimale solo sotto l'assunzione di distribuzione gaussiana.\n",
    "\n",
    "Per ottenere uno stimatore migliore, dobbiamo abbandonare l'assunzione gaussiana e considerare una distribuzione con code pesanti. Una scelta comune è la **distribuzione t di Student**, che modella le code pesanti tramite il parametro $$\\nu$$.\n",
    "\n",
    "### Funzione di densità di probabilità (pdf)\n",
    "\n",
    "Assumendo che il termine residuo segua una distribuzione t multivariata, la pdf è:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{\\Gamma\\left(\\frac{\\nu + N}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)(\\nu \\pi)^{N/2} |\\Sigma|^{1/2}} \\left(1 + \\frac{1}{\\nu}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right)^{-\\frac{\\nu + N}{2}}\n",
    "$$\n",
    "\n",
    "Dove:\n",
    "\n",
    "- $\\mu$ è il parametro di posizione (media se $\\nu > 1$),\n",
    "- $\\Sigma$ è la matrice di dispersione (la covarianza è $\\frac{\\nu}{\\nu - 2} \\Sigma$ se $\\nu > 2$),\n",
    "- $\\nu$ è il numero di gradi di libertà (controlla la pesantezza delle code).\n",
    "\n",
    "### Formulazione del problema MLE\n",
    "\n",
    "Dato un insieme di osservazioni $x_1, \\dots, x_T$, il problema di stima diventa:\n",
    "\n",
    "$$\n",
    "\\min_{\\mu, \\Sigma, \\nu} \\log \\det(\\Sigma) + \\frac{\\nu + N}{T} \\sum_{t=1}^{T} \\log\\left(1 + \\frac{1}{\\nu}(x_t - \\mu)^T \\Sigma^{-1} (x_t - \\mu)\\right)\n",
    "$$\n",
    "\n",
    "Per semplicità, si fissa $$\\nu = 4$$. Le equazioni a punto fisso risultanti sono:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{T} \\sum_{t=1}^{T} w_t(\\mu, \\Sigma) \\cdot x_t \\quad \\text{con} \\quad w_t(\\mu, \\Sigma) = \\frac{\\nu + N}{\\nu + (x_t - \\mu)^T \\Sigma^{-1} (x_t - \\mu)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{T} \\sum_{t=1}^{T} w_t(\\mu, \\Sigma) \\cdot (x_t - \\mu)(x_t - \\mu)^T\n",
    "$$\n",
    "\n",
    "### Algoritmo Iterativo (Algoritmo 3.1)\n",
    "\n",
    "\n",
    "**Algoritmo 3.1: Metodo MM per la stima ML con code pesanti**\n",
    "\n",
    "1. Scegli un punto iniziale $(\\mu_0, \\Sigma_0)$\n",
    "2. Imposta $k \\leftarrow 0$\n",
    "3. Ripeti fino a convergenza:\n",
    "   - Calcola i pesi:  \n",
    "     $w_t^{(k)} = \\frac{\\nu + N}{\\nu + (x_t - \\mu_k)^T \\Sigma_k^{-1} (x_t - \\mu_k)}$\n",
    "   - Aggiorna la media:  \n",
    "     $\\mu_{k+1} = \\frac{1}{T} \\sum_{t=1}^{T} w_t^{(k)} x_t$\n",
    "   - Aggiorna la covarianza:  \n",
    "     $\\Sigma_{k+1} = \\frac{1}{T} \\sum_{t=1}^{T} w_t^{(k)} (x_t - \\mu_{k+1})(x_t - \\mu_{k+1})^T$\n",
    "   - $k \\leftarrow k + 1$\n",
    "\n",
    "\n",
    "Questo metodo è robusto agli outlier e converge rapidamente (in 3–5 iterazioni in media).\n",
    "\n",
    "![stime](./images/scatter-plots-t-MLE-1.png)\n",
    "\n",
    "\n",
    "La **Figura** illustra la robustezza dello **stimatore** illustra la robust ML a code pesanti, confrontato con lo **stimatore ML gaussiano**, in presenza di dati con code pesanti e valori anomali.  \n",
    "La differenza osservata è piuttosto marcata e dovrebbe rappresentare un **serio campanello d’allarme** per i professionisti che utilizzano la **matrice di covarianza campionaria** quando si trovano a lavorare con dati a code pesanti.\n",
    "\n",
    "### Stimatori robusti\n",
    "Cosa succede se la distribuzione reale che genera i dati si discosta leggermente da quella assunta – tipicamente gaussiana? Gli stimatori che non sono molto sensibili ai valori anomali o alla contaminazione della distribuzione sono generalmente chiamati stimatori robusti.\n",
    "\n",
    "La robustezza degli stimatori può essere misurata oggettivamente in diversi modi; in particolare, con la funzione di influenza, che misura l'effetto di una leggera deviazione della distribuzione reale da quella assunta, e con il punto di rottura, che rappresenta la minima frazione di dati contaminati in grado di rendere lo stimatore inutilizzabile.\n",
    "\n",
    "Come già discusso nella Sezione 3.4, gli stimatori campionari o gli stimatori ML gaussiani non sono robusti rispetto a deviazioni dalla distribuzione gaussiana. È ben noto che sono molto sensibili alle code della distribuzione (Huber, 1964; Maronna, 1976). Infatti, la loro funzione di influenza è non limitata, il che significa che una contaminazione infinitesimale può avere un'influenza arbitrariamente grande. \n",
    "\n",
    "Inoltre, un singolo punto contaminato può compromettere la media campionaria o la matrice di covarianza campionaria, ovvero il punto di rottura è pari a $1/T$\n",
    "\n",
    "Per confronto, la mediana ha un punto di rottura di circa 0,5, cioè è necessario contaminare più del 50% dei punti per comprometterla. \n",
    "\n",
    "D'altra parte, come verrà ulteriormente approfondito nella prossima sezione, gli stimatori ML a code pesanti della Sezione 3.5.2 possono essere considerati stimatori robusti.\n",
    "\n",
    "Alcuni riferimenti classici sulla stima robusta sono Huber (1964) per il caso univariato e Maronna (1976) per il caso multivariato, mentre rassegne più moderne includono Maronna et al. (2006), Huber (2011), Wiesel e Zhang (2014), e il Capitolo 4 in Zoubir et al. (2018).\n",
    "\n",
    "### Stimatori M\n",
    "\n",
    "Il termine **stimatori M** per la stima robusta risale agli anni '60 (Huber, 1964).  In sintesi, gli **stimatori M** per i parametri di **posizione** e **dispersione**,  $\\mu$ e $\\Sigma$, sono definiti dalle seguenti **equazioni a punto fisso**:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "  \\frac{1}{T} \\sum_{t=1}^T u_1\\left(\\sqrt{(x_t - \\mu)^\\T \\Sigma^{-1}(x_t - \\mu)}\\right) (x_t - \\mu) &= 0,\\\\\n",
    "  \\frac{1}{T}\\sum_{t=1}^T u_2\\left((x_t - \\mu)^\\T\\Sigma^{-1}(x_t - \\mu)\\right) (x_t - \\mu)(x_t - \\mu)^T &= \\Sigma,\n",
    "  \\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Dove $u_1(\\cdot)$ e $u_2(\\cdot)$ sono funzioni di peso che soddisfano alcune condizioni (Maronna, 1976; Maronna et al., 2006).\n",
    "\n",
    "Gli **stimatori M** sono una generalizzazione degli stimatori di massima verosimiglianza e possono essere considerati come la media campionaria pesata e la matrice di covarianza campionaria pesata. In termini di robustezza, possiedono una desiderabile funzione di influenza limitata, sebbene il punto di rottura sia ancora relativamente basso (Maronna, 1976; Maronna et al., 2006). Altri stimatori, come l'**ellissoide di volume minimo** e il **determinante di covarianza minimo**, presentano punti di rottura più elevati.\n",
    "\n",
    "Gli **stimatori ML gaussiani** possono essere ottenuti dagli **stimatori M** nella (3.9) con la scelta banale delle funzioni di peso  \n",
    "$$u_1(s) = u_2(s) = 1$$.\n",
    "\n",
    "Una scelta comune per ottenere stimatori robusti consiste nel definire le funzioni di peso a partire dalla **funzione $\\psi$ di Huber**:  $\\psi(z, k) = \\max(-k, \\min(z, k))$  \n",
    "dove $k$ è una costante positiva che limita l’argomento $z$ superiormente e inferiormente, come segue (Maronna, 1976):  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  u_1(s) &= \\psi(z,k)/s,\\\\\n",
    "  u_2(s) &= \\psi(z,k^2)/(\\beta s),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "dove $\\beta$ è una costante scelta opportunamente.\n",
    "È interessante notare che gli **stimatori M** nella (3.9) si riducono agli **stimatori ML a code pesanti** derivati nella (3.7) con la seguente scelta:\n",
    "\n",
    "$$\n",
    "u_1(s) = u_2(s^2) = \\frac{\\nu + N}{\\nu + s^2}.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Stimatore di Tyler\n",
    "\n",
    "Nel 1987, Tyler propose uno stimatore per la matrice di dispersione (che è proporzionale alla matrice di covarianza) per distribuzioni a code pesanti (Tyler, 1987). L’idea è molto semplice e ingegnosa, come descritto di seguito. È interessante notare che lo **stimatore di Tyler** può essere considerato la versione più robusta di uno **stimatore M**.\n",
    "\n",
    "Supponiamo che la variabile casuale $\\mathbf{x}$ segua una distribuzione ellittica a media nulla – il che significa che la distribuzione dipende da $\\mathbf{x}$ attraverso il termine  \n",
    "$\\mathbf{x}^T \\Sigma^{-1} \\mathbf{x}$.  \n",
    "Se la media non è nulla, allora deve essere stimata con uno stimatore di posizione, come descritto nella Sezione 3.3, e poi sottratta dalle osservazioni affinché abbiano media nulla.\n",
    "\n",
    "L’idea chiave è normalizzare le osservazioni:\n",
    "\n",
    "$$\n",
    "s_t = \\frac{x_t}{\\|x_t\\|_2}\n",
    "$$\n",
    "\n",
    "e poi utilizzare la massima verosimiglianza (ML) basata su questi punti normalizzati. Il fatto sorprendente è che la **pdf** (funzione di densità di probabilità) dei punti normalizzati può essere derivata analiticamente – nota come **distribuzione angolare** – come segue:\n",
    "\n",
    "$$\n",
    "f(s) \\propto \\frac{1}{\\sqrt{| \\Sigma|}} \\left(s^T\\Sigma^{-1}s\\right)^{-N/2},\n",
    "$$\n",
    "\n",
    "# Informazioni Preliminari: Shrinkage, Modelli Fattoriali e Black–Litterman\n",
    "\n",
    "Tutti gli stimatori finora esaminati in questo capitolo si basano esclusivamente sui $T$ punti dati storici $x_1, \\dots, x_T$.\n",
    "\n",
    "Purtroppo, in molti contesti pratici, come discusso nella Sezione 3.2, il numero di osservazioni non è sufficientemente grande per ottenere una stima accurata dei parametri del modello con un errore abbastanza piccolo. Ricercatori e professionisti hanno trascorso decenni cercando di affrontare questo problema e ideando una varietà di meccanismi per migliorare gli stimatori. La strategia di base consiste nell'incorporare in qualche modo qualsiasi informazione preliminare disponibile.\n",
    "\n",
    "## Tre approcci principali per integrare informazioni preliminari:\n",
    "\n",
    "- **Shrinkage**: per incorporare conoscenze a priori sui parametri sotto forma di obiettivi;\n",
    "- **Modellazione Fattoriale**: per incorporare informazioni strutturali;\n",
    "- **Black–Litterman**: per combinare i dati con opinioni discrezionali.\n",
    "\n",
    "> Ognuno di questi approcci meriterebbe un intero capitolo – se non un intero libro – ma qui ci limiteremo a fornire una panoramica introd approfondire.\n",
    "\n",
    "## Shrinkage\n",
    "\n",
    "Lo *shrinkage* è una tecnica molto diffusa per ridurre l’errore di stima introducendo un bias nello stimatore. In statistica, questa idea risale al 1955 con la pubblicazione fondamentale di Stein (Stein, 1955). In ambito finanziario, è stata resa popolare dall’applicazione allo *shrinkage* della matrice di covarianza nei primi anni 2000 (Ledoit e Wolf, 2004) ed è ora trattata in molti libri di testo (Meucci, 2005) e rassegne (Bun et al., 2017).\n",
    "\n",
    "L’errore quadratico medio (*Mean Squared Error*, MSE) di uno stimatore può essere scomposto in: il bias e la varianza. Questo è un concetto fondamentale nella teoria della stima, noto come *bias–variance trade-off* (Kay, 1993; Scharf, 1991). Matematicamente, per un parametro dato  $\\theta$  e uno stimatore  $\\hat{\\theta}$, il compromesso bias–varianza si esprime come:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  MSE(\\hat{\\theta})\n",
    "  & \\triangleq E\\left[\\big\\|\\hat{\\theta} - \\theta \\big\\|^2\\right]\\\\\n",
    "  & = E\\left[\\big\\|\\hat{\\theta} - E\\big[\\hat{\\theta}\\big]\\big\\|^2\\right] +\n",
    "      \\big\\|E\\big[\\hat{\\theta}\\big] - \\theta\\big\\|^2\\\\\n",
    "  & = Var\\big(\\hat{\\theta}\\big) + Bias^2\\big(\\hat{\\theta}\\big).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Regimi di Campionamento e Shrinkage\n",
    "\n",
    "Nel regime a campione ridotto (cioè quando il numero di osservazioni è piccolo), la principale fonte di errore proviene dalla **varianza** dello stimatore (poiché lo stimatore si basa su un numero limitato di campioni casuali). Nel regime a campione ampio, invece, ci si può aspettare che la varianza dello stimatore si riduca e che il **bias** domini l’errore complessivo.\n",
    "\n",
    "Tradizionalmente, gli stimatori non distorti (*unbiased*) sono sempre stati considerati desiderabili. Con sorpresa della comunità statistica, Stein dimostrò nel 1955, in un articolo fondamentale (Stein, 1955), che può essere vantaggioso introdurre un certo bias per ottenere un errore complessivo minore. Questo può essere implementato **riducendo lo stimatore verso un valore obiettivo noto**.\n",
    "\n",
    "Matematicamente, per uno stimatore $\\hat{\\theta}$ e un valore obiettivo $\\theta_{\\text{tgt}}$ (cioè l'informazione preliminare), uno stimatore *shrinkage* è definito come:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}^\\text{sh} = (1 - \\rho)\\, \\hat{\\theta} + \\rho\\, \\theta^\\text{tgt},\n",
    "$$\n",
    "\n",
    "dove  $\\rho$ (con $0 \\leq \\rho \\leq 1$) è il **parametro di compromesso dello shrinkage**, detto anche **fattore di shrinkage**.\n",
    "\n",
    "In pratica, ci sono due questioni importanti nell'implementazione dello shrinkage:\n",
    "\n",
    "1. **La scelta del valore obiettivo**  \n",
    "   $\\theta_{\\text{tgt}}$, che rappresenta l’informazione preliminare e, in un contesto finanziario, può derivare da opinioni discrezionali sul mercato;\n",
    "\n",
    "2. **La scelta del fattore di shrinkage**  \n",
    "   $\\rho$, che può sembrare un problema banale, ma in realtà ha generato una vasta letteratura.\n",
    "\n",
    "Sebbene la scelta del valore obiettivo sia importante, è forse sorprendente notare che la parte più critica è la **scelta del fattore di shrinkage** $\\rho$. Il motivo è che, indipendentemente da quanto sia mal scelto il target, una scelta adeguata di $\\rho$ può sempre pesare correttamente il contributo del target.\n",
    "\n",
    "#### Filosofie per la scelta di $\\rho$:\n",
    "\n",
    "- **Scelta empirica** basata sulla *cross-validation*;\n",
    "- **Scelta analitica** basata su metodi matematici sofisticati.\n",
    "\n",
    "Nel nostro contesto di dati finanziari, il parametro  $\\theta$ può rappresentare il **vettore delle medie**  $\\mu$  \n",
    "oppure la **matrice di covarianza**  $\\Sigma$,  e lo stimatore  $\\hat{\\theta}$ può essere, ad esempio, la **media campionaria** o la **matrice di covarianza campionaria**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09866b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab2c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
